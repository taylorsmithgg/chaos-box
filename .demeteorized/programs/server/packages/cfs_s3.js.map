{"version":3,"file":"/packages/cfs:s3.js","sources":["cfs:s3/s3.server.js","cfs:s3/s3.upload.stream2.js"],"names":[],"mappings":";;;;;;;;;;;;;;;;;AAAA,8B;AACA,6B;;AAEA,+B;AACA,a;AACA,gB;AACA,oB;AACA,iB;AACA,gB;AACA,uB;AACA,W;AACA,e;AACA,iB;AACA,e;AACA,oB;AACA,qB;AACA,qB;AACA,gB;AACA,e;AACA,gB;AACA,W;AACA,oB;AACA,E;AACA,2B;AACA,Q;AACA,S;AACA,W;AACA,iB;AACA,uB;AACA,oB;AACA,oB;AACA,kB;AACA,e;AACA,gB;AACA,Y;AACA,qB;AACA,c;AACA,iB;AACA,kB;AACA,Q;AACA,a;AACA,yB;AACA,iB;AACA,2B;AACA,E;;AAEA,G;AACA,U;AACA,e;AACA,wC;AACA,0B;AACA,iD;AACA,+C;AACA,oG;AACA,2G;AACA,yE;AACA,wF;AACA,iN;AACA,4E;AACA,iE;AACA,E;AACA,8E;AACA,Q;AACA,G;AACA,uC;AACA,kB;AACA,qC;AACA,yD;;AAEA,0B;;AAEA,6D;AACA,8B;AACA,oD;AACA,qC;AACA,+B;AACA,K;AACA,mC;AACA,oB;AACA,K;AACA,U;AACA,gB;AACA,G;;AAEA,8B;AACA,c;AACA,wE;;AAEA,4C;;AAEA,yC;AACA,uD;;AAEA,yC;AACA,mB;AACA,4B;AACA,iC;AACA,qC;AACA,mB;AACA,c;;AAEA,0D;AACA,6E;AACA,gC;AACA,oE;;AAEA,sB;AACA,qC;;AAEA,+C;AACA,2B;AACA,gC;AACA,wB;AACA,mD;AACA,qD;AACA,4C;;AAEA,oC;AACA,wD;;AAEA,0D;AACA,8F;AACA,M;AACA,kD;;AAEA,kC;AACA,uB;AACA,6B;AACA,S;;AAEA,M;AACA,wE;AACA,8E;AACA,+D;AACA,+C;AACA,mD;AACA,8B;;AAEA,gC;AACA,kD;AACA,O;;AAEA,yC;AACA,6B;AACA,oC;AACA,iC;AACA,+C;AACA,8B;;AAEA,oB;AACA,uC;AACA,uB;AACA,8B;AACA,yB;AACA,uB;AACA,kB;;AAEA,2C;AACA,M;AACA,yC;;AAEA,uB;AACA,uB;AACA,6B;AACA,0B;AACA,gC;AACA,S;AACA,M;AACA,uB;AACA,6E;AACA,K;AACA,K;AACA,E;;;;;;;;;;;;;;;;;;;AC5KA,8C;;AAEA,iC;AACA,qF;AACA,2C;;AAEA,+D;AACA,mB;AACA,mD;AACA,E;;AAEA,wB;AACA,gE;AACA,kB;;AAEA,0C;AACA,8B;AACA,kC;AACA,K;;AAEA,qB;AACA,iB;AACA,uB;AACA,uB;AACA,+B;AACA,6B;AACA,+B;AACA,sB;AACA,yD;;AAEA,yB;AACA,wB;;AAEA,4E;AACA,kB;AACA,yC;AACA,mD;AACA,qC;AACA,oC;AACA,iC;AACA,Y;AACA,mC;AACA,iB;AACA,K;AACA,I;;AAEA,gD;AACA,oD;AACA,wD;;AAEA,8E;AACA,+E;AACA,kB;AACA,4C;AACA,0D;AACA,qD;AACA,Y;AACA,4C;AACA,yB;AACA,K;AACA,I;;AAEA,6E;AACA,2C;AACA,qC;AACA,0D;AACA,qB;AACA,0D;AACA,0D;AACA,yD;AACA,O;AACA,I;;AAEA,uC;AACA,4B;AACA,qB;AACA,uC;AACA,O;AACA,iC;AACA,8B;AACA,wB;AACA,mC;AACA,yB;AACA,iB;AACA,wE;AACA,S;AACA,S;AACA,K;AACA,K;;AAEA,wC;AACA,qC;AACA,kE;AACA,K;AACA,yB;AACA,qD;AACA,sC;;;AAGA,kE;AACA,wC;;AAEA,8B;AACA,0C;;AAEA,sB;AACA,qB;AACA,2B;AACA,4B;AACA,sB;AACA,kC;AACA,kC;AACA,+B;AACA,2B;AACA,0C;AACA,mB;AACA,O;;AAEA,e;AACA,uC;AACA,c;AACA,mC;AACA,8C;AACA,qC;AACA,4B;AACA,sC;AACA,U;;AAEA,mC;AACA,mC;AACA,4B;AACA,uC;AACA,qC;AACA,oC;AACA,W;;AAEA,0E;AACA,+E;AACA,oD;AACA,uD;AACA,gD;AACA,gC;AACA,wC;AACA,kC;AACA,4B;AACA,wC;AACA,8B;AACA,0B;AACA,a;AACA,qC;AACA,qB;AACA,6C;AACA,oB;AACA,mD;AACA,6B;AACA,8C;AACA,e;AACA,0C;AACA,iC;AACA,mC;AACA,oC;AACA,iB;AACA,a;;AAEA,a;AACA,S;AACA,O;AACA,O;;AAEA,+B;AACA,6B;AACA,I;;AAEA,6D;AACA,4D;AACA,a;AACA,uB;AACA,qC;AACA,Y;AACA,0B;AACA,wC;;AAEA,8B;AACA,kD;AACA,yE;AACA,+B;AACA,0B;AACA,O;;AAEA,K;AACA,K;;AAEA,+B;AACA,qB;AACA,E","sourcesContent":["// We use the official aws sdk\nAWS = Npm.require('aws-sdk');\n\nvar validS3ServiceParamKeys = [\n  'endpoint',\n  'accessKeyId',\n  'secretAccessKey',\n  'sessionToken',\n  'credentials',\n  'credentialProvider',\n  'region',\n  'maxRetries',\n  'maxRedirects',\n  'sslEnabled',\n  'paramValidation',\n  'computeChecksums',\n  's3ForcePathStyle',\n  'httpOptions',\n  'apiVersion',\n  'apiVersions',\n  'logger',\n  'signatureVersion'\n];\nvar validS3PutParamKeys = [\n  'ACL',\n  'Body',\n  'Bucket',\n  'CacheControl',\n  'ContentDisposition',\n  'ContentEncoding',\n  'ContentLanguage',\n  'ContentLength',\n  'ContentMD5',\n  'ContentType',\n  'Expires',\n  'GrantFullControl',\n  'GrantRead',\n  'GrantReadACP',\n  'GrantWriteACP',\n  'Key',\n  'Metadata',\n  'ServerSideEncryption',\n  'StorageClass',\n  'WebsiteRedirectLocation'\n];\n\n/**\n * @public\n * @constructor\n * @param {String} name - The store name\n * @param {Object} options\n * @param {String} options.region - Bucket region\n * @param {String} options.bucket - Bucket name\n * @param {String} [options.accessKeyId] - AWS IAM key; required if not set in environment variables\n * @param {String} [options.secretAccessKey] - AWS IAM secret; required if not set in environment variables\n * @param {String} [options.ACL='private'] - ACL for objects when putting\n * @param {String} [options.folder='/'] - Which folder (key prefix) in the bucket to use\n * @param {Function} [options.beforeSave] - Function to run before saving a file from the server. The context of the function will be the `FS.File` instance we're saving. The function may alter its properties.\n * @param {Number} [options.maxTries=5] - Max times to attempt saving a file\n * @returns {FS.StorageAdapter} An instance of FS.StorageAdapter.\n *\n * Creates an S3 store instance on the server. Inherits from FS.StorageAdapter\n * type.\n */\nFS.Store.S3 = function(name, options) {\n  var self = this;\n  if (!(self instanceof FS.Store.S3))\n    throw new Error('FS.Store.S3 missing keyword \"new\"');\n\n  options = options || {};\n\n  // Determine which folder (key prefix) in the bucket to use\n  var folder = options.folder;\n  if (typeof folder === \"string\" && folder.length) {\n    if (folder.slice(0, 1) === \"/\") {\n      folder = folder.slice(1);\n    }\n    if (folder.slice(-1) !== \"/\") {\n      folder += \"/\";\n    }\n  } else {\n    folder = \"\";\n  }\n\n  var bucket = options.bucket;\n  if (!bucket)\n    throw new Error('FS.Store.S3 you must specify the \"bucket\" option');\n\n  var defaultAcl = options.ACL || 'private';\n\n  // Remove serviceParams from SA options\n // options = _.omit(options, validS3ServiceParamKeys);\n\n  var serviceParams = FS.Utility.extend({\n    Bucket: bucket,\n    region: null, //required\n    accessKeyId: null, //required\n    secretAccessKey: null, //required\n    ACL: defaultAcl\n  }, options);\n\n  // Whitelist serviceParams, else aws-sdk throws an error\n  // XXX: I've commented this at the moment... It stopped things from working\n  // we have to check up on this\n  // serviceParams = _.pick(serviceParams, validS3ServiceParamKeys);\n\n  // Create S3 service\n  var S3 = new AWS.S3(serviceParams);\n\n  return new FS.StorageAdapter(name, options, {\n    typeName: 'storage.s3',\n    fileKey: function(fileObj) {\n      // Lookup the copy\n      var info = fileObj && fileObj._getInfo(name);\n      // If the store and key is found return the key\n      if (info && info.key) return info.key;\n\n      var filename = fileObj.name();\n      var filenameInStore = fileObj.name({store: name});\n\n      // If no store key found we resolve / generate a key\n      return fileObj.collectionName + '/' + fileObj._id + '-' + (filenameInStore || filename);\n    },\n    createReadStream: function(fileKey, options) {\n\n      return S3.createReadStream({\n        Bucket: bucket,\n        Key: folder + fileKey\n      });\n\n    },\n    // Comment to documentation: Set options.ContentLength otherwise the\n    // indirect stream will be used creating extra overhead on the filesystem.\n    // An easy way if the data is not transformed is to set the\n    // options.ContentLength = fileObj.size ...\n    createWriteStream: function(fileKey, options) {\n      options = options || {};\n\n      if (options.contentType) {\n        options.ContentType = options.contentType;\n      }\n\n      // We dont support array of aliases\n      delete options.aliases;\n      // We dont support contentType\n      delete options.contentType;\n      // We dont support metadata use Metadata?\n      delete options.metadata;\n\n      // Set options\n      var options = FS.Utility.extend({\n        Bucket: bucket,\n        Key: folder + fileKey,\n        fileKey: fileKey,\n        ACL: defaultAcl\n      }, options);\n\n      return S3.createWriteStream(options);\n    },\n    remove: function(fileKey, callback) {\n\n      S3.deleteObject({\n        Bucket: bucket,\n        Key: folder + fileKey\n      }, function(error) {\n        callback(error, !error);\n      });\n    },\n    watch: function() {\n      throw new Error(\"S3 storage adapter does not support the sync option\");\n    }\n  });\n};\n","var Writable = Npm.require('stream').Writable;\n\n// This is based on the code from\n// https://github.com/nathanpeck/s3-upload-stream/blob/master/lib/s3-upload-stream.js\n// But much is rewritten and adapted to cfs\n\nAWS.S3.prototype.createReadStream = function(params, options) {\n  // Simple wrapper\n  return this.getObject(params).createReadStream();\n};\n\n// Extend the AWS.S3 API\nAWS.S3.prototype.createWriteStream = function(params, options) {\n  var self = this;\n\n  //Create the writeable stream interface.\n  var writeStream = Writable({\n    highWaterMark: 4194304 // 4 MB\n  });\n\n  var partNumber = 1;\n  var parts = [];\n  var receivedSize = 0;\n  var uploadedSize = 0;\n  var currentChunk = Buffer(0);\n  var maxChunkSize = 5242880;\n  var multipartUploadID = null;\n  var waitingCallback;\n  var fileKey = params && (params.fileKey || params.Key);\n\n  // Clean up for AWS sdk\n  delete params.fileKey;\n\n  // This small function stops the write stream until we have connected with\n  // the s3 server\n  var runWhenReady = function(callback) {\n    // If we dont have a upload id we are not ready\n    if (multipartUploadID === null) {\n      // We set the waiting callback\n      waitingCallback = callback;\n    } else {\n      // No problem - just continue\n      callback();\n    }\n  };\n\n  //Handler to receive data and upload it to S3.\n  writeStream._write = function (chunk, enc, next) {\n    currentChunk = Buffer.concat([currentChunk, chunk]);\n\n    // If the current chunk buffer is getting to large, or the stream piped in\n    // has ended then flush the chunk buffer downstream to S3 via the multipart\n    // upload API.\n    if(currentChunk.length > maxChunkSize) {\n      // Make sure we only run when the s3 upload is ready\n      runWhenReady(function() { flushChunk(next); });\n    } else {\n      // We dont have to contact s3 for this\n      runWhenReady(next);\n    }\n  };\n\n  // Overwrite the end method so that we can hijack it to flush the last part\n  // and then complete the multipart upload\n  var _originalEnd = writeStream.end;\n  writeStream.end = function (chunk, encoding, callback) {\n    // Call the super\n    _originalEnd.call(this, chunk, encoding, function () {\n      // Make sure we only run when the s3 upload is ready\n      runWhenReady(function() { flushChunk(callback); });\n    });\n  };\n\n  writeStream.on('error', function () {\n    if (multipartUploadID) {\n      if (FS.debug) {\n        console.log('SA S3 - ERROR!!');\n      }\n      self.abortMultipartUpload({\n        Bucket: params.Bucket,\n        Key: params.Key,\n        UploadId: multipartUploadID\n      }, function (err) {\n        if(err) {\n          console.error('SA S3 - Could not abort multipart upload', err)\n        }\n      });\n    }\n  });\n\n  var flushChunk = function (callback) {\n    if (multipartUploadID === null) {\n      throw new Error('Internal error multipartUploadID is null');\n    }\n    // Get the chunk data\n    var uploadingChunk = Buffer(currentChunk.length);\n    currentChunk.copy(uploadingChunk);\n\n\n    // Store the current part number and then increase the counter\n    var localChunkNumber = partNumber++;\n\n    // We add the size of data\n    receivedSize += uploadingChunk.length;\n\n    // Upload the part\n    self.uploadPart({\n      Body: uploadingChunk,\n      Bucket: params.Bucket,\n      Key: params.Key,\n      UploadId: multipartUploadID,\n      PartNumber: localChunkNumber\n    }, function (err, result) {\n      // Call the next data\n      if(typeof callback === 'function') {\n        callback();\n      }\n\n      if(err) {\n        writeStream.emit('error', err);\n      } else {\n        // Increase the upload size\n        uploadedSize += uploadingChunk.length;\n        parts[localChunkNumber-1] = {\n          ETag: result.ETag,\n          PartNumber: localChunkNumber\n        };\n\n        // XXX: event for debugging\n        writeStream.emit('chunk', {\n          ETag: result.ETag,\n          PartNumber: localChunkNumber,\n          receivedSize: receivedSize,\n          uploadedSize: uploadedSize\n        });\n\n        // The incoming stream has finished giving us all data and we have\n        // finished uploading all that data to S3. So tell S3 to assemble those\n        // parts we uploaded into the final product.\n        if(writeStream._writableState.ended === true &&\n                uploadedSize === receivedSize) {\n          // Complete the upload\n          self.completeMultipartUpload({\n            Bucket: params.Bucket,\n            Key: params.Key,\n            UploadId: multipartUploadID,\n            MultipartUpload: {\n              Parts: parts\n            }\n          }, function (err, result) {\n            if(err) {\n              writeStream.emit('error', err);\n            } else {\n              // Emit the cfs end event for uploads\n              if (FS.debug) {\n                console.log('SA S3 - DONE!!');\n              }\n              writeStream.emit('stored', {\n                fileKey: fileKey,\n                size: uploadedSize,\n                storedAt: new Date()\n              });\n            }\n\n          });\n        }\n      }\n    });\n\n    // Reset the current buffer\n    currentChunk = Buffer(0);\n  };\n\n  //Use the S3 client to initialize a multipart upload to S3.\n  self.createMultipartUpload( params, function (err, data) {\n    if(err) {\n      // Emit the error\n      writeStream.emit('error', err);\n    } else {\n      // Set the upload id\n      multipartUploadID = data.UploadId;\n\n      // Call waiting callback\n      if (typeof waitingCallback === 'function') {\n        // We call the waiting callback if any now since we established a\n        // connection to the s3\n        waitingCallback();\n      }\n\n    }\n  });\n\n  // We return the write stream\n  return writeStream;\n};\n"]}